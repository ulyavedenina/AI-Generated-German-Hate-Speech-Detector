{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11614054",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "To successfully run the following notebook, in the output of the first cell you need to insert a valid Llama2 access token. This token can be requested on their model card (https://huggingface.co/meta-llama/Llama-2-13b-hf).\n",
    "\n",
    "A GPU is highly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212a36f8-af3b-4833-ab00-4290068dc274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e392e57416144309a708ddf73292714c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\n",
    "import bitsandbytes as bnb\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c53543-ba44-47e8-bb89-f9c3d7d29661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Count GPUs\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d150ca5-5cc9-4a96-8141-7b09889d4ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "Available GPUs after setting CUDA_VISIBLE_DEVICES: 1\n",
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Count GPUs\n",
    "print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES to use only GPU 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Count GPUs again to see if the change took effect\n",
    "print(\"Available GPUs after setting CUDA_VISIBLE_DEVICES:\", torch.cuda.device_count())\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda:1\") \n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9afbd37f-b4e8-4331-a1c1-7018bb70f3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change model_id to either 7b or 13b\n",
    "model_id = \"meta-llama/Llama-2-13b-hf\" # \"meta-llama/Llama-2-7b-hf\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb5168d-fc3b-46e2-9034-7049b39ecb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_train = datasets.load_from_disk(\"../../../dataset/bot_ds\")[\"train\"] # change \n",
    "no_bot_train = datasets.load_from_disk(\"../../../dataset/no_bot_ds\")[\"train\"] # change \n",
    "bot_train = bot_train.add_column(\"output\", [1]*len(bot_train))  \n",
    "no_bot_train = no_bot_train.add_column(\"output\", [0]*len(no_bot_train))\n",
    "train_dataset = datasets.concatenate_datasets([bot_train, no_bot_train])\n",
    "train_dataset = train_dataset.shuffle(10)\n",
    "train_sentences, train_labels = train_dataset['text'], train_dataset['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2dcf8dc-50c2-4295-92f9-5d7e10f4ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_val = datasets.load_from_disk(\"../../../dataset/bot_ds\")[\"val\"] # change \n",
    "no_bot_val = datasets.load_from_disk(\"../../../dataset/no_bot_ds\")[\"val\"] # change \n",
    "bot_val = bot_val.add_column(\"output\", [1]*len(bot_val))  \n",
    "no_bot_val = no_bot_val.add_column(\"output\", [0]*len(no_bot_val)) \n",
    "val_dataset = datasets.concatenate_datasets([bot_val, no_bot_val])\n",
    "val_dataset = val_dataset.shuffle(10)\n",
    "val_sentences, val_labels = val_dataset['text'], val_dataset['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdbbec8c-90a8-48a6-a77d-3af8eec50d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "train_encodings = tokenizer(train_sentences, truncation=True, max_length=512, padding='max_length', return_attention_mask=True)\n",
    "val_encodings = tokenizer(val_sentences, truncation=True, max_length=512, padding='max_length', return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d88dc363-a8d2-4bf7-bd35-8342861aa8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79de59f5-7fb0-46a2-ab0d-b89271eaf8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "val_dataset = CustomDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e0b203-1efb-4a82-b249-e324570a4a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12688"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97e91dff-bb0f-4c14-96cf-d804b880977c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3173"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4940bec9-34cb-4b0d-baf7-06d8ec7d624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0567d0e98d496489c3cfd42a9f9667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-13b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "### Prepare model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float32  #bfloat\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, \n",
    "                                                        quantization_config=bnb_config,\n",
    "                                                        trust_remote_code=True,\n",
    "                                                        num_labels=len(set(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ed53ed0-04d6-421f-ba58-5da21e391c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 5120)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7fe9105-09f5-4a0f-8911-f027876f59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e60a232d-944e-4d6b-9c0c-2cb8254c865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4953943-4834-4f19-b076-e20bc4b57abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a1fa0b6-214b-4e4f-8842-8be0b37777c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47bde4f4-940a-4108-9441-4c55c56af77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_proj', 'k_proj', 'down_proj', 'up_proj', 'v_proj', 'gate_proj', 'o_proj']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules = find_all_linear_names(model)\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2a1cf75-a454-4f8b-ab02-84c370658fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 62597120 || all params: 6570752000 || trainable%: 0.9526629524291892\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3172' max='3172' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3172/3172 3:29:13, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.131400</td>\n",
       "      <td>0.070825</td>\n",
       "      <td>0.981406</td>\n",
       "      <td>0.996139</td>\n",
       "      <td>0.966896</td>\n",
       "      <td>0.981300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.046219</td>\n",
       "      <td>0.988969</td>\n",
       "      <td>0.991217</td>\n",
       "      <td>0.986883</td>\n",
       "      <td>0.989045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66e83d5d-20cffef3714c3324555f5eec;d6493273-3c6a-4caf-90f2-52669c0c3756)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-13b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-13b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-13b-hf.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Llama-2-13b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66e855e3-7c8c9ffc38fddf07536d4513;90753c26-ef8a-499f-97af-52d972d696c2)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-13b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-13b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-13b-hf.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Llama-2-13b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         2.0\n",
      "  total_flos               = 925724286GF\n",
      "  train_loss               =      0.0753\n",
      "  train_runtime            =  3:29:18.60\n",
      "  train_samples_per_second =       2.021\n",
      "  train_steps_per_second   =       0.253\n",
      "Memory Usage: 7.31 GB\n",
      "{'train_runtime': 12558.608, 'train_samples_per_second': 2.021, 'train_steps_per_second': 0.253, 'total_flos': 9.93988883471401e+17, 'train_loss': 0.07526023932092578, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "model_directory = \"./models/\"\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=64, \n",
    "    target_modules=modules, \n",
    "    lora_dropout=0.4, \n",
    "    bias=\"none\", \n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "   # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(labels, preds, average='binary')\n",
    "    recall = recall_score(labels, preds, average='binary')\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    logging_dir=model_directory, \n",
    "    output_dir=model_directory,    # change     \n",
    "    evaluation_strategy='epoch', \n",
    "    load_best_model_at_end = True,\n",
    "    logging_steps = 100, \n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    save_total_limit=2,\n",
    "    save_strategy= \"epoch\",\n",
    "    optim=\"paged_adamw_32bit\", \n",
    "    learning_rate = 2e-5,\n",
    "    fp16 = True, \n",
    "    push_to_hub=False,\n",
    ")\n",
    "print_trainable_parameters(model)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. please re-enable for inference!\n",
    "do_train = True\n",
    "\n",
    "def log_memory_usage():\n",
    "    mem_alloc = torch.cuda.memory_allocated() / 1024**3  # Memory usage in GB\n",
    "    print(f\"Memory Usage: {mem_alloc:.2f} GB\")\n",
    "\n",
    "# Launch training and log metrics\n",
    "print(\"Training...\")\n",
    "\n",
    "\n",
    "if do_train:\n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    log_memory_usage()\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f11002ee-8904-40eb-bf6b-f020822efbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving last checkpoint of the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66e859c7-702537c14567d7747945e896;631a3ada-4e4a-4860-b784-07b8079049d3)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-13b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-13b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-13b-hf.\n",
      "  warnings.warn(\n",
      "/home/stjowort/botox/AP4/venv-AP4/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Llama-2-13b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "print(\"Saving last checkpoint of the model...\")\n",
    "trainer.model.save_pretrained(model_directory, safe_serialization=False)\n",
    "\n",
    "# Free memory for merging weights\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05217390-458e-453b-8997-7910ca0bc4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6953d40bc2ae41ec88de3b94afe04d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-13b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "\n",
    "PEFT_MODEL = model_directory #+ \"best-model\"\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    num_labels=2\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"}) \n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86863fdb-44ad-4d35-8e2c-9fdccd2cab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Testdata \n",
    "bot_test = datasets.load_from_disk(\"../../../dataset/bot_ds\")[\"test\"]\n",
    "no_bot_test = datasets.load_from_disk(\"../../../dataset/no_bot_ds\")[\"test\"]\n",
    "bot_test = bot_test.add_column(\"output\", [1]*len(bot_test))\n",
    "no_bot_test = no_bot_test.add_column(\"output\", [0]*len(no_bot_test))\n",
    "test_dataset = datasets.concatenate_datasets([bot_test, no_bot_test])\n",
    "test_dataset = test_dataset.shuffle(10)\n",
    "test_sentences, test_labels = test_dataset['text'], test_dataset['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "463b54fd-8529-4fa3-8655-5ec3cf42529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3197/3197 [06:05<00:00,  8.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.957556270096463"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "preds = []\n",
    "for sentence in tqdm(test_sentences, desc=\"Evaluation\"):\n",
    "    input = tokenizer(sentence, return_tensors='pt', truncation=True, max_length=512)\n",
    "    pred = model(**input) \n",
    "    preds.append(pred.logits.argmax().item())\n",
    "\n",
    "metric.compute(predictions=preds, references=test_labels, average='binary')[\"f1\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
