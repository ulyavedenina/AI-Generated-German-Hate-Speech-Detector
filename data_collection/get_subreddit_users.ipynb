{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filterning step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove duplicates and save to reddit_comments_cleaned.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit_comments_kommunismus.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "\n",
    "def remove_duplicate_items(_api_data, _key):\n",
    "    print(\"Initial items in list: {}\".format(len(_api_data)))\n",
    "    unique_elements = []\n",
    "    cleaned_data = []\n",
    "    keys = []\n",
    "    for i, j in enumerate(_api_data):\n",
    "        if _api_data[i][_key] not in unique_elements:\n",
    "            unique_elements.append(_api_data[i][_key])\n",
    "            keys.append(i)\n",
    "\n",
    "    for key in keys:\n",
    "        cleaned_data.append(_api_data[key])\n",
    "\n",
    "    print(\n",
    "        \"Total duplicates removed: {}, Total items: {}, Final items:{}\".format(\n",
    "            (len(_api_data) - len(unique_elements)),\n",
    "            len(_api_data), len(unique_elements)))\n",
    "    print(\"Final items in list: {}\".format(len(cleaned_data)))\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial items in list: 140888\n",
      "Total duplicates removed: 78796, Total items: 140888, Final items:62092\n",
      "Final items in list: 62092\n"
     ]
    }
   ],
   "source": [
    "unique_data = remove_duplicate_items(json_data, \"comment_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit_comments_cleaned_kommunismus.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unique_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load offensive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the keywords from the text file\n",
    "with open('offensive_words.txt', 'r') as f:\n",
    "    keywords = [line.strip().lower() for line in f if line.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_comments = []\n",
    "for unique_comment in unique_data:\n",
    "\n",
    "    comment_text = unique_comment.get('comment', '').lower()\n",
    "    if any(keyword in comment_text for keyword in keywords):\n",
    "        keyword_comments.append(unique_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13533"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyword_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract comments with offensive words and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered comments saved to the file\n"
     ]
    }
   ],
   "source": [
    "with open('reddit_comments_keyword_kommunismus.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unique_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f'Filtered comments saved to the file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract usernames from the filtered comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13533\n"
     ]
    }
   ],
   "source": [
    "username_list = []\n",
    "for keyword_comment in keyword_comments:\n",
    "    username = keyword_comment.get('username', '')\n",
    "    username_list.append(username)\n",
    "\n",
    "print(len(username_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fafikommander'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the usernames to the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit_users_kommunismus.txt', 'a') as f:\n",
    "    for user in username_list:\n",
    "\n",
    "     f.write(f\"{user}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
